\chapter{Data warehouse management and data mining}

\label{ch:database}
Up to this point, our data was stored in the \textit{data} directory in CSV format. The role of the database managing container is to take this data, ensure its correctness, and upload it to a MySQL database. MySQL was chosen because of relative simplicity paired with decent scalability. Additionaly, the only library used in relation to the database is \textit{mysql-connector-python}. \par
A database is introduced because although having the data stored securly in compressed files on the host filesystem may seem good, we would like to be able to use the data to gain information in the problem domain. Having created the tables in our database, we can use SQL to retrieve information from them; and powerful, multi-stage queries may provide unexpectedly novel knowledge from simple entities. \par
A data warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management's decision-making process \cite{dataWarehouseMining}. In order to be congruent to the definition above, presented system must generate a collection of data that is:

\begin{enumerate}
    \item subject-oriented --- related to questions in a single problem domain, e.g. gathering vending statistics and product information in supermarkets to improve sales through better advertising.
    \item integrated --- combined from possibly several sources into a single, uniform warehouse, which can be accessed by any department within the organization, and the data is ready to be structured into spreadsheets or tables.
    \item time-variant --- contained and consistent within a time period; when loaded daily, weekly, or hourly does not change within that period.
    \item nonvolatile --- locked from changes to the gathered data, which once entered into the warehouse, does not change.
\end{enumerate}

\section{Database managing}

The role of managing the connected database with regards to updating the data falls to the \textit{database\_manager} container. It's internal structure and configuration is similar to the \textit{data\_gathering} container, described in chapter \ref{ch:gathering}.

\subsection{Program configuration}
The configurable global variables are stored in file \textit{config.py}. There are three subgroups inside:

\begin{enumerate}
    \item Variables connected to a single run of the code. \par
          Database connector and MySQL connection configuration is stored here, together with names of log files and the checksum of data directory.
    \item Fixed variables. \par
          \texttt{CONTAINER\_DELAY = 30 \\ NAME = `database-manager'}
    \item SQL static queries. \par
          This section has two variables: \texttt{DROP} and \texttt{CREATE} which are dictionaries holding pairs of table names and appropriate SQL queries to drop and create them.
\end{enumerate}


\subsection{Used services}
The program, similar to data-gathering program, uses four services, three of which are shared among both of them with minor changes.

\begin{itemize}
    \item \textit{Database service}.
          Responsible for handling the connection to the database. It provides access to the data, as well as functions responsible for updating the tables.

    \item \textit{Data service}.
          Here the service is 53 lines long, with functions detecting the presence of flags file, isolating the data into a Docker volume, loading SQL entities into a Dataframe, and decompressing the CSV files into a container directory from which MySQL can load all records simultaneously.

    \item \textit{Flags service}.
          This service provides the utility for managing datasets versions across containers; here we use it to check whether current dataset is indeed complete (in \textit{validated-checksums.sha1}), then to see whether it differs from the one that is currently in the database (\textit{database-checksum.sha1}). If it detects a disparity, the new data will be loaded, and its checksum saved to the latter file.

    \item \textit{Logs service}.
          The use of this service is the same as described in the analogous section \ref{s:used_services} in the previous chapter.
\end{itemize}

\subsection{Run scheduling}
After the initial setup, the program enters a control loop deciding whtether to commence an update or not. First, it compares the checksums of local files and database content. If the newest data is already in the database, the program sleeps for 30 minutes. Otherwise, it tries to verify the new data by checking it against the \textit{validated-checksums.sha1} file. When the dataset is concluded to be new, but not yet verified, the program waits 15 minutes. After uploading the data, a 60-minute pause is initiated and the program begins the check again.

\subsection{Tables from the staging area}
When the data is gathered by the first container, it holds information about five entities: \textit{date, card, seller, card\_stats} and \textit{sale\_offer}. The data types of their attributes are dictated by the type of information they convey and are defined as described in subsection \ref{ss:base_tables}. \par
In order to upload the data to the database, uncompressed CSV files are copied to \textit{/var/lib/mysql-files} directory inside the container. This allows for the \texttt{LOAD FILE} query to be executed, drastically improving the processing speed.


\section{Data mining}
When the data is in the database, it can be used to gain insights about issues that concern us. Large tables can be reduced if the historical data is not needed every time, e.g. the \textit{sale\_offers} table, having 90 thousand new records every day and over 10 million overall, can be used to extract just the last two weeks of sales. This two-week time period information is needed by other queries and creating such a sliced table improves the access time greatly. The other increase in speed comes from a similar table, where only the sale offers from the current day are stored. \par
The data is also read into the mining module and transformed into Dataframes, table-like objects provided by the \textit{pandas} library. These provide a multitude of transformations and method to be applied to the data, which the author uses to extract valuable information about the problem domain.


\subsection{Program configuration}
The config.py file holds variables related to the database connection and log files, similar to the \textit{database\_manager}. Fixed variables comprise container delay, here set to 60 seconds, and the project name. There is one control variable to be set when the \textit{sale\_offer} table has more than 10 million records, to change the data retrieval method.


\subsection{Used services}
\begin{itemize}
    \item \textit{Database service}.
          Responsible for handling the connection to the database, checking its state, the newest date and creating the helper tables through SQL queries.

    \item \textit{Mining service}.
          Loads the data saved in the database and performs exploratory analysis, saving the results to the \textit{analysis} directory.

    \item \textit{Flags service}.
          Here the service is used to ensure that there is no new verified dataset to be uploaded to the database --- in that case the program waits 5 minutes for the database manager to load the data.

    \item \textit{Logs service}.
          The use of this service is the same as described in the analogous section \ref{s:used_services} in the previous chapter.
\end{itemize}

\subsection{Run scheduling}
The run is halted for 5 minutes, whenever new data is about to be uploaded to the database. When the \textit{database\_manager} container's job is done, the database connection is established and the number of tables is checked. If it's less than 5, the analysis cannot be performed, and the program is set up to wait 120 seconds and query the database again. Should the number of tables be correct, the data transformation run is commenced. After finishing the program waits 60 minutes and begins the routine again. This way, data mining is not dependent on the database content, which may change 23 hours within a day, resulting in 23 correct separate analyses.

\subsection{Data analysis}
\label{ss:data_analysis}
Apart from creating the helper tables, an analysis is performed on the collected data. Appropriate directory is created and the five dataframes undergo many transformations to extract the following information:

\begin{itemize}
    \item actual rarity, number of cards in each rarity class;
    \item amount of sellers with public address, by seller type;
    \item site popularity from the number of new users each year;
    \item number of new sellers by year and seller type;
    \item ranking of countries with the most sellers;
    \item price statistics trends, average price history;
    \item average price statistics distributions;
    \item the number of available marketable items over time;
    \item new attribute indicating market rarity, inverse ratio of card availability to mean availability;
    \item heatmap of correlations between prices, available items and the calculated market rarity;
    \item a plot of each card's 50 best sale prices and when they occured with the best 30 and 10 offers marked differently (in total 264 plots in the \textit{analysis/cards} subdirectory).
\end{itemize}
